import os, sys
import h5py 
import rdkit.Chem as Chem 
import rdkit.Chem.AllChem as AllChem
import random 
import numpy as np
from multiprocessing import Pool, cpu_count, Process, Manager, Queue, JoinableQueue
from scipy import sparse
import cPickle as pickle 

'''
This script is used to generate an .h5 file containing all the fingerprints 
needed to train the model, so they do not have to be generated on the fly.

It needs to be called with a command-line argument containing the path to the 
.txt file; in our case, this is the txt file generated by get_reaxys_data.py

This script uses a pool of workers to generate fingerprints to speed the 
process up a little.
'''

path = sys.argv[1]

if not os.path.isfile(path):
	quit('Need to specify a file to read from')

data = []
with open(path, 'r') as f:
    for line in f:
        rex, n, rxn_id, tmp_num, tmp_id = line.strip("\r\n").split(' ')
        r,p = rex.split('>>')
        if ('.' in p) or (not p):
            continue # do not allow multiple products or none
        n = int(n)
        data.append((p, r, n, rxn_id, tmp_num, tmp_id))
random.seed(123) # always use this seed to get the same split, arbitrarly chosen
random.shuffle(data)
print('Read and shuffled {} total data'.format(len(data)))
with open(path + '.data_pkl', 'w') as fid:
    pickle.dump(data, fid, -1)



FP_len = 1024
FP_rad = 2
def mol_to_fp(mol, radius=FP_rad, nBits=FP_len, convFunc=sparse.lil_matrix):
    if mol is None:
        return convFunc((nBits,), dtype=np.bool)
    return convFunc(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits, useChirality=True), dtype=np.bool)

def smi_to_fp(smi, radius=FP_rad, nBits=FP_len, convFunc=sparse.lil_matrix):
    if not smi:
        return convFunc((nBits,), dtype=np.bool)
    return mol_to_fp(Chem.MolFromSmiles(smi), radius, nBits, convFunc)

def chunks(l, n):
    """Yield successive n-sized chunks from l."""
    for i in range(0, len(l), n):
        yield l[i:i + n]

FPs = sparse.lil_matrix((len(data), FP_len), dtype=bool)
pool = Pool(24)
# for lst in chunks(range(ALREADY_DONE, len(data)), 50000): # 50k chunks
for lst in chunks(range(len(data)), 50000): # 50k chunks
	for i, fp in enumerate(pool.imap(smi_to_fp, [data[i][0] for i in lst], chunksize=500)):
		FPs[lst[i],:] = fp

	print('latest index done: %i/%i' % (lst[-1]+1, len(data)))

# Convert to csr
FPs = FPs.tocsr()

with open(path + '.fp_pkl', 'w') as fid:
    pickle.dump(FPs, fid)