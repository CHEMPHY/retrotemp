import os, sys
import h5py 
import rdkit.Chem as Chem 
import rdkit.Chem.AllChem as AllChem
import random 
import numpy as np
from multiprocessing import Pool, cpu_count, Process, Manager, Queue, JoinableQueue
from scipy import sparse
import cPickle as pickle 
from functools import partial 

'''
This script is used to generate an .h5 file containing all the fingerprints 
needed to train the model, so they do not have to be generated on the fly.

It needs to be called with a command-line argument containing the path to the 
.txt file; in our case, this is the txt file generated by get_reaxys_data.py

This script uses a pool of workers to generate fingerprints to speed the 
process up a little.

**FORWARD DIRECTION**
'''



path = sys.argv[1]
outpath = sys.argv[2]

if not os.path.isfile(path):
    quit('Need to specify a file to read from')

data = []
with open(path, 'r') as f:
    for line in f:
        rex, rgts, cats, slvs, T, NYD, rxn_id, tmp_num, tmp_id = line.strip("\r\n").split(' ')
        r,p = rex.split('>>')
        if ('.' in p) or (not p):
            continue # do not allow multiple products or none
        data.append((p, r, rgts, cats, slvs, float(T), float(NYD), rxn_id, int(tmp_num), tmp_id))
random.seed(123) # always use this seed to get the same split, arbitrarly chosen
random.shuffle(data)
print('Read and shuffled {} total data'.format(len(data)))
with open(path + '.data_pkl', 'w') as fid:
    pickle.dump(data, fid, -1)



RCT_FP_len = 2048
RGT_FP_len = 512
CAT_FP_len = 512
SLV_FP_len = 256
FP_rad = 2
def mol_to_fp(mol, radius=FP_rad, nBits=RCT_FP_len, convFunc=sparse.lil_matrix):
    if mol is None:
        return convFunc((1, nBits), dtype=np.bool)
    return convFunc(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits, useChirality=True), dtype=np.bool)

def smi_to_fp(smi, radius=FP_rad, nBits=RCT_FP_len, convFunc=sparse.lil_matrix):
    if not smi:
        return convFunc((1, nBits), dtype=np.bool)
    return mol_to_fp(Chem.MolFromSmiles(smi), radius, nBits, convFunc)

def chunks(l, n):
    """Yield successive n-sized chunks from l."""
    for i in range(0, len(l), n):
        yield l[i:i + n]

FPs = sparse.lil_matrix((len(data), RCT_FP_len), dtype=bool)
RGTs = sparse.lil_matrix((len(data), RGT_FP_len), dtype=bool)
CATs = sparse.lil_matrix((len(data), CAT_FP_len), dtype=bool)
SLVs = sparse.lil_matrix((len(data), SLV_FP_len), dtype=bool)
Ts = np.zeros((len(data),), dtype=float)

# Set different FP lengths
FPs_smi_to_fp = partial(smi_to_fp, nBits=RCT_FP_len)
RGTs_smi_to_fp = partial(smi_to_fp, nBits=RGT_FP_len)
CATs_smi_to_fp = partial(smi_to_fp, nBits=CAT_FP_len)
SLVs_smi_to_fp = partial(smi_to_fp, nBits=SLV_FP_len)


pool = Pool(24)
# for lst in chunks(range(ALREADY_DONE, len(data)), 50000): # 50k chunks
for lst in chunks(range(len(data)), 50000): # 50k chunks
    for i, fp in enumerate(pool.imap(FPs_smi_to_fp, [data[i][1] for i in lst], chunksize=500)):
        FPs[lst[i],:] = fp
    for i, fp in enumerate(pool.imap(RGTs_smi_to_fp, [data[i][2] for i in lst], chunksize=500)):
        RGTs[lst[i],:] = fp
    for i, fp in enumerate(pool.imap(CATs_smi_to_fp, [data[i][3] for i in lst], chunksize=500)):
        CATs[lst[i],:] = fp
    for i, fp in enumerate(pool.imap(SLVs_smi_to_fp, [data[i][4] for i in lst], chunksize=500)):
        SLVs[lst[i],:] = fp
    for i, T in enumerate([data[i][5] for i in lst]):
        Ts[lst[i]] = T

    print('latest index done: %i/%i' % (lst[-1]+1, len(data)))

# Convert to csr
FPs = FPs.tocsr()
RGTs = RGTs.tocsr()
CATs = CATs.tocsr()
SLVs = SLVs.tocsr()
Ts[Ts == -1] = 20 # change default temperature

with open(outpath, 'w') as fid:
    pickle.dump(FPs, fid, -1)
    pickle.dump(RGTs, fid, -1)
    pickle.dump(CATs, fid, -1)
    pickle.dump(SLVs, fid, -1)
    pickle.dump(Ts, fid, -1)